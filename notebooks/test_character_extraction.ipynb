{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Character Detection モデルのテスト\n",
    "\n",
    "このnotebookでは、学習済みの文字位置検出（Character Detection）モデルの性能評価と結果の可視化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ryo/project/kuzushiji-vision-lightning\n",
      "Current working directory: /home/ryo/project/kuzushiji-vision-lightning\n"
     ]
    }
   ],
   "source": [
    "%cd ~/project/kuzushiji-vision-lightning\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPUが利用可能な場合は使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. 実験設定の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model file not found at experiments/character_detection/yolo12x/weights/best.pt\n",
      "Please update experiment_dir with the correct path after training is complete.\n"
     ]
    }
   ],
   "source": [
    "# 実験ディレクトリの設定（学習完了後に適切なパスに変更してください）\n",
    "# 例: \"experiments/character_detection/20250101_120000\"\n",
    "YOLO_MODEL = \"yolo12x\"\n",
    "\n",
    "experiment_dir = f\"experiments/character_detection/{YOLO_MODEL}\"  # 実際の学習結果ディレクトリに変更\n",
    "model_path = os.path.join(experiment_dir, \"weights/best.pt\")\n",
    "config_path = os.path.join(experiment_dir, \"config.yaml\")\n",
    "\n",
    "# 設定ファイルの読み込み\n",
    "def load_yaml(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# 学習済みモデルの確認\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model found: {model_path}\")\n",
    "    if os.path.exists(config_path):\n",
    "        config = load_yaml(config_path)\n",
    "        print(\"Configuration loaded successfully\")\n",
    "    else:\n",
    "        print(f\"Warning: Config file not found at {config_path}\")\n",
    "        config = {}\n",
    "else:\n",
    "    print(f\"Warning: Model file not found at {model_path}\")\n",
    "    print(\"Please update experiment_dir with the correct path after training is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5409 test images\n",
      "Found 5409 image-label pairs\n"
     ]
    }
   ],
   "source": [
    "# テストデータのパス設定\n",
    "test_images_dir = \"data/yolo_dataset_character_detection/\"\n",
    "test_labels_dir = \"data/yolo_dataset_character_detection/\"\n",
    "\n",
    "def load_yolo_labels(label_path):\n",
    "    \"\"\"YOLOラベルファイルを読み込む\n",
    "    \n",
    "    Args:\n",
    "        label_path (str): ラベルファイルのパス\n",
    "        \n",
    "    Returns:\n",
    "        list: [class_id, x_center, y_center, width, height] のリスト\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center = float(parts[1])\n",
    "                    y_center = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "                    labels.append([class_id, x_center, y_center, width, height])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Label file not found: {label_path}\")\n",
    "    return labels\n",
    "\n",
    "def yolo_to_xyxy(box, img_width, img_height):\n",
    "    \"\"\"YOLO形式（正規化済み）からxyxy形式に変換\n",
    "    \n",
    "    Args:\n",
    "        box: [class_id, x_center, y_center, width, height]\n",
    "        img_width: 画像幅\n",
    "        img_height: 画像高さ\n",
    "        \n",
    "    Returns:\n",
    "        list: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    class_id, x_center, y_center, width, height = box\n",
    "    \n",
    "    # 正規化座標を実座標に変換\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "    \n",
    "    # 中心座標と幅高さから左上右下座標に変換\n",
    "    x1 = x_center - width / 2\n",
    "    y1 = y_center - height / 2\n",
    "    x2 = x_center + width / 2\n",
    "    y2 = y_center + height / 2\n",
    "    \n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "# テスト画像とラベルの対応を作成\n",
    "test_data = []\n",
    "image_files = glob.glob(os.path.join(test_images_dir, \"**\", \"*.jpg\"), recursive=True) + glob.glob(os.path.join(test_images_dir, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "print(f\"Found {len(image_files)} test images\")\n",
    "\n",
    "for image_path in image_files:\n",
    "    image_name = os.path.basename(image_path)\n",
    "    label_name = os.path.splitext(image_name)[0] + \".txt\"\n",
    "    label_path = os.path.join(os.path.dirname(image_path).replace(\"images\", \"labels\"), label_name)\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        test_data.append({\n",
    "            'image_path': image_path,\n",
    "            'label_path': label_path,\n",
    "            'image_name': os.path.join(*image_path.split(\"/\")[-3:])\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(test_data)} image-label pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. モデルのセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading from /home/ryo/.config/Ultralytics/settings.json: \"No Ultralytics setting 'openvino_msg'. \\nView Ultralytics Settings with 'yolo settings' or at '/home/ryo/.config/Ultralytics/settings.json'\\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\"\n",
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/ryo/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Model file not found: experiments/character_detection/yolo12x/weights/best.pt\n",
      "Please ensure the model has been trained and the path is correct.\n",
      "Loading pre-trained YOLOv8n model for demonstration...\n",
      "Demo model loaded (replace with your trained model)\n"
     ]
    }
   ],
   "source": [
    "# YOLOモデルの読み込み\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 学習済みモデルの読み込み\n",
    "try:\n",
    "    if os.path.exists(model_path):\n",
    "        model = YOLO(model_path)\n",
    "        model.to(device)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        print(\"Please ensure the model has been trained and the path is correct.\")\n",
    "        # デモンストレーション用にプリトレーニングモデルを使用（実際のテストでは削除）\n",
    "        print(\"Loading pre-trained YOLOv8n model for demonstration...\")\n",
    "        model = YOLO('yolov8n.pt')\n",
    "        model.to(device)\n",
    "        print(\"Demo model loaded (replace with your trained model)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. 推論と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"IoUを計算する\n",
    "    \n",
    "    Args:\n",
    "        box1: [x1, y1, x2, y2]\n",
    "        box2: [x1, y1, x2, y2]\n",
    "        \n",
    "    Returns:\n",
    "        float: IoUスコア\n",
    "    \"\"\"\n",
    "    # 交差領域の座標を計算\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # 交差領域の面積を計算\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # それぞれのボックスの面積を計算\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # IoUを計算\n",
    "    union = box1_area + box2_area - intersection\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def evaluate_predictions(predictions, ground_truth, iou_threshold=0.5):\n",
    "    \"\"\"予測結果を評価する\n",
    "    \n",
    "    Args:\n",
    "        predictions: 予測された矩形のリスト [[x1, y1, x2, y2, conf], ...]\n",
    "        ground_truth: 正解の矩形のリスト [[x1, y1, x2, y2], ...]\n",
    "        iou_threshold: IoUの閾値\n",
    "        \n",
    "    Returns:\n",
    "        dict: 評価指標\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = len(ground_truth)\n",
    "    \n",
    "    # 各予測に対して最も近い正解を探す\n",
    "    matched_gt = set()\n",
    "    for pred in predictions:\n",
    "        best_iou = 0 \n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for i, gt in enumerate(ground_truth):\n",
    "            if i in matched_gt:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(pred[:4], gt)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = i\n",
    "        \n",
    "        if best_iou >= iou_threshold:\n",
    "            true_positives += 1\n",
    "            matched_gt.add(best_gt_idx)\n",
    "            false_negatives -= 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    # 評価指標の計算\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ad744f167448ff9014a467f7021d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/5409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 推論実行\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m600\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 予測結果の取得\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(predictions, \u001b[33m'\u001b[39m\u001b[33mboxes\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m predictions.boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:550\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:214\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:295\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m.setup_model(model)\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_txt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:255\u001b[39m, in \u001b[36mBasePredictor.setup_source\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.imgsz = check_imgsz(\u001b[38;5;28mself\u001b[39m.args.imgsz, stride=\u001b[38;5;28mself\u001b[39m.model.stride, min_dim=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28mself\u001b[39m.transforms = (\n\u001b[32m    247\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m    248\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    254\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28mself\u001b[39m.source_type = \u001b[38;5;28mself\u001b[39m.dataset.source_type\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mself\u001b[39m.source_type.stream\n\u001b[32m    264\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.screenshot\n\u001b[32m    265\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset) > \u001b[32m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33mvideo_flag\u001b[39m\u001b[33m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[32m    267\u001b[39m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/ultralytics/data/build.py:251\u001b[39m, in \u001b[36mload_inference_source\u001b[39m\u001b[34m(source, batch, vid_stride, buffer)\u001b[39m\n\u001b[32m    249\u001b[39m     dataset = LoadScreenshots(source)\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m from_img:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     dataset = \u001b[43mLoadPilAndNumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    253\u001b[39m     dataset = LoadImagesAndVideos(source, batch=batch, vid_stride=vid_stride)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/ultralytics/data/loaders.py:485\u001b[39m, in \u001b[36mLoadPilAndNumpy.__init__\u001b[39m\u001b[34m(self, im0)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# use `image{i}.jpg` when Image.filename returns an empty path.\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;28mself\u001b[39m.paths = [\u001b[38;5;28mgetattr\u001b[39m(im, \u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, im \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(im0)]\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m \u001b[38;5;28mself\u001b[39m.im0 = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m im0]\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m.mode = \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28mself\u001b[39m.bs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.im0)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/ultralytics/data/loaders.py:496\u001b[39m, in \u001b[36mLoadPilAndNumpy._single_check\u001b[39m\u001b[34m(im)\u001b[39m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m im.mode != \u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    495\u001b[39m         im = im.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     im = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m[:, :, ::-\u001b[32m1\u001b[39m]\n\u001b[32m    497\u001b[39m     im = np.ascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/PIL/Image.py:756\u001b[39m, in \u001b[36mImage.__array_interface__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    754\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.tobytes(\u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m756\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m new[\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m], new[\u001b[33m\"\u001b[39m\u001b[33mtypestr\u001b[39m\u001b[33m\"\u001b[39m] = _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/PIL/Image.py:805\u001b[39m, in \u001b[36mImage.tobytes\u001b[39m\u001b[34m(self, encoder_name, *args)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_name == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args == ():\n\u001b[32m    803\u001b[39m     encoder_args = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.width == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.height == \u001b[32m0\u001b[39m:\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/kuzushiji-vision-lightning/.venv/lib/python3.12/site-packages/PIL/ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# テスト画像に対して推論と評価を実行\n",
    "results = []\n",
    "metrics = []\n",
    "\n",
    "if model is not None:\n",
    "    # 限定的なテスト（最初の10枚の画像のみ）\n",
    "    #test_subset = test_data[:10] if len(test_data) > 10 else test_data\n",
    "    test_subset = test_data\n",
    "    \n",
    "    for data in tqdm(test_subset, desc=\"Testing\"):\n",
    "        image_path = data['image_path']\n",
    "        label_path = data['label_path']\n",
    "        image_name = data['image_name']\n",
    "        \n",
    "        # 画像の読み込み\n",
    "        image = Image.open(image_path)\n",
    "        img_width, img_height = image.size\n",
    "        \n",
    "        # グラウンドトゥルースの読み込み\n",
    "        gt_labels = load_yolo_labels(label_path)\n",
    "        gt_boxes = [yolo_to_xyxy(label, img_width, img_height) for label in gt_labels]\n",
    "        \n",
    "        # 推論実行\n",
    "        with torch.no_grad():\n",
    "            predictions = model.predict(\n",
    "                image, conf=0.25, iou=0.7, verbose=False, max_det=600)[0]\n",
    "        \n",
    "        # 予測結果の取得\n",
    "        if hasattr(predictions, 'boxes') and predictions.boxes is not None:\n",
    "            pred_boxes = predictions.boxes.data.cpu().numpy()\n",
    "            # YOLOの出力形式: [x1, y1, x2, y2, confidence, class]\n",
    "            pred_boxes_with_conf = pred_boxes.tolist()\n",
    "        else:\n",
    "            pred_boxes_with_conf = []\n",
    "        \n",
    "        # 評価\n",
    "        metric = evaluate_predictions(pred_boxes_with_conf, gt_boxes)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "        # 結果を保存\n",
    "        results.append({\n",
    "            'image_name': image_name,\n",
    "            'image_path': image_path,\n",
    "            'image_size': (img_width, img_height),\n",
    "            'predictions': pred_boxes_with_conf,\n",
    "            'ground_truth': gt_boxes,\n",
    "            'metrics': metric\n",
    "        })\n",
    "    \n",
    "    # 全体の評価指標を計算\n",
    "    if metrics:\n",
    "        overall_metrics = {\n",
    "            'precision': np.mean([m['precision'] for m in metrics]),\n",
    "            'recall': np.mean([m['recall'] for m in metrics]),\n",
    "            'f1': np.mean([m['f1'] for m in metrics])\n",
    "        }\n",
    "        \n",
    "        print(\"\\nOverall Metrics:\")\n",
    "        for k, v in overall_metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(\"No results to evaluate\")\n",
    "else:\n",
    "    print(\"Model is not loaded. Cannot perform evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(result, figsize=(15, 10)):\n",
    "    \"\"\"検出結果を可視化する\n",
    "    \n",
    "    Args:\n",
    "        result: 結果辞書\n",
    "        figsize: 図のサイズ\n",
    "    \"\"\"\n",
    "    image = Image.open(result['image_path'])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # 左側: Ground Truth\n",
    "    ax1.imshow(image)\n",
    "    for box in result['ground_truth']:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                               linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "    \n",
    "    ax1.set_title(f\"Ground Truth\\nCount: {len(result['ground_truth'])}\")\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # 右側: Predictions\n",
    "    ax2.imshow(image)\n",
    "    for box in result['predictions']:\n",
    "        if len(box) >= 5:  # [x1, y1, x2, y2, conf, class]\n",
    "            x1, y1, x2, y2, conf = box[:5]\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, \n",
    "                                   linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax2.add_patch(rect)\n",
    "            # ax2.text(x1, y1 - 5, f'{conf:.2f}', color='red', fontsize=8, \n",
    "            #        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax2.set_title(f\"Predictions\\nCount: {len(result['predictions'])}\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # 全体のタイトル\n",
    "    fig.suptitle(f\"Image: {result['image_name']}\\n\"\n",
    "                f\"F1: {result['metrics']['f1']:.4f}, \"\n",
    "                f\"Precision: {result['metrics']['precision']:.4f}, \"\n",
    "                f\"Recall: {result['metrics']['recall']:.4f}\", fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 結果の可視化（モデルが読み込まれ、結果がある場合のみ）\n",
    "if results:\n",
    "    # 最良と最悪のケースを可視化\n",
    "    sorted_results = sorted(results, key=lambda x: x['metrics']['f1'], reverse=True)\n",
    "    \n",
    "    print(\"Best Case:\")\n",
    "    for i, case in enumerate(sorted_results[:3]):\n",
    "        print(f\"\\nBest Case {i+1}:\")\n",
    "        visualize_results(case)\n",
    "    \n",
    "    if len(sorted_results) > 3:\n",
    "        print(\"\\nWorst Case:\")\n",
    "        for i, case in enumerate(sorted_results[-4:]):\n",
    "            print(f\"\\nWorst Case {i+1}:\")\n",
    "            visualize_results(case)\n",
    "    \n",
    "    # いくつかのランダムなケースも表示\n",
    "    import random\n",
    "    if len(results) > 2:\n",
    "        print(\"\\\\nRandom Cases:\")\n",
    "        random_cases = random.sample(results, min(3, len(results)))\n",
    "        for i, case in enumerate(random_cases):\n",
    "            print(f\"\\\\nRandom Case {i+1}:\")\n",
    "            visualize_results(case)\n",
    "else:\n",
    "    print(\"No results to visualize. Please ensure the model is trained and loaded correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. エラー分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エラー分析とパフォーマンス統計\n",
    "if results:\n",
    "    # F1スコアの分布を可視化\n",
    "    f1_scores = [r['metrics']['f1'] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # F1スコアのヒストグラム\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(f1_scores, bins=10, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of F1 Scores')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 予測数 vs 正解数の散布図\n",
    "    pred_counts = [len(r['predictions']) for r in results]\n",
    "    gt_counts = [len(r['ground_truth']) for r in results]\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(gt_counts, pred_counts, alpha=0.6)\n",
    "    plt.plot([0, max(max(gt_counts), max(pred_counts))], [0, max(max(gt_counts), max(pred_counts))], 'r--', alpha=0.5)\n",
    "    plt.title('Predictions vs Ground Truth Count')\n",
    "    plt.xlabel('Ground Truth Count')\n",
    "    plt.ylabel('Prediction Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 困難なケースの分析\n",
    "    difficult_cases = [r for r in results if r['metrics']['f1'] < 0.5]\n",
    "    good_cases = [r for r in results if r['metrics']['f1'] > 0.8]\n",
    "    \n",
    "    print(f\"\\nPerformance Analysis:\")\n",
    "    print(f\"Total test cases: {len(results)}\")\n",
    "    print(f\"Good cases (F1 > 0.8): {len(good_cases)} ({len(good_cases)/len(results)*100:.1f}%)\")\n",
    "    print(f\"Difficult cases (F1 < 0.5): {len(difficult_cases)} ({len(difficult_cases)/len(results)*100:.1f}%)\")\n",
    "    print(f\"Average F1 score: {np.mean(f1_scores):.4f}\")\n",
    "    print(f\"Median F1 score: {np.median(f1_scores):.4f}\")\n",
    "    print(f\"Standard deviation: {np.std(f1_scores):.4f}\")\n",
    "    \n",
    "    # 困難なケースの特徴分析\n",
    "    if difficult_cases:\n",
    "        print(f\"\\nDifficult Cases Analysis:\")\n",
    "        for case in difficult_cases[:3]:  # 最初の3つの困難なケースを分析\n",
    "            print(f\"\\nImage: {case['image_name']}\")\n",
    "            print(f\"  F1: {case['metrics']['f1']:.4f}\")\n",
    "            print(f\"  Predictions: {len(case['predictions'])}, Ground Truth: {len(case['ground_truth'])}\")\n",
    "            print(f\"  Image size: {case['image_size']}\")\n",
    "            print(f\"  TP: {case['metrics']['true_positives']}, \"\n",
    "                  f\"FP: {case['metrics']['false_positives']}, \"\n",
    "                  f\"FN: {case['metrics']['false_negatives']}\")\n",
    "    \n",
    "    # パフォーマンス統計の詳細\n",
    "    print(f\"\\nDetailed Statistics:\")\n",
    "    precisions = [r['metrics']['precision'] for r in results]\n",
    "    recalls = [r['metrics']['recall'] for r in results]\n",
    "    \n",
    "    print(f\"Precision - Mean: {np.mean(precisions):.4f}, Std: {np.std(precisions):.4f}\")\n",
    "    print(f\"Recall - Mean: {np.mean(recalls):.4f}, Std: {np.std(recalls):.4f}\")\n",
    "    print(f\"F1 - Mean: {np.mean(f1_scores):.4f}, Std: {np.std(f1_scores):.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results available for analysis. Please ensure the model is properly trained and loaded.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## yolo11n\n",
    "Performance Analysis:\n",
    "Total test cases: 713\n",
    "Good cases (F1 > 0.8): 706 (99.0%)\n",
    "Difficult cases (F1 < 0.5): 3 (0.4%)\n",
    "Average F1 score: 0.9532\n",
    "Median F1 score: 0.9652\n",
    "Standard deviation: 0.0646\n",
    "\n",
    "Detailed Statistics:\n",
    "Precision - Mean: 0.9519, Std: 0.0637\n",
    "Recall - Mean: 0.9572, Std: 0.0740\n",
    "F1 - Mean: 0.9532, Std: 0.0646\n",
    "\n",
    "## yolo11l\n",
    "Performance Analysis:\n",
    "Total test cases: 713\n",
    "Good cases (F1 > 0.8): 707 (99.2%)\n",
    "Difficult cases (F1 < 0.5): 2 (0.3%)\n",
    "Average F1 score: 0.9636\n",
    "Median F1 score: 0.9732\n",
    "Standard deviation: 0.0490\n",
    "\n",
    "Detailed Statistics:\n",
    "Precision - Mean: 0.9675, Std: 0.0337\n",
    "Recall - Mean: 0.9631, Std: 0.0638\n",
    "F1 - Mean: 0.9636, Std: 0.0490\n",
    "\n",
    "## yolo11x\n",
    "Performance Analysis:\n",
    "Total test cases: 713\n",
    "Good cases (F1 > 0.8): 709 (99.4%)\n",
    "Difficult cases (F1 < 0.5): 0 (0.0%)\n",
    "Average F1 score: 0.9713\n",
    "Median F1 score: 0.9801\n",
    "Standard deviation: 0.0329\n",
    "\n",
    "Detailed Statistics:\n",
    "Precision - Mean: 0.9756, Std: 0.0302\n",
    "Recall - Mean: 0.9689, Std: 0.0482\n",
    "F1 - Mean: 0.9713, Std: 0.0329\n",
    "\n",
    "## yolo11x max_det=700\n",
    "Performance Analysis:\n",
    "Total test cases: 713\n",
    "Good cases (F1 > 0.8): 710 (99.6%)\n",
    "Difficult cases (F1 < 0.5): 0 (0.0%)\n",
    "Average F1 score: 0.9757\n",
    "Median F1 score: 0.9811\n",
    "Standard deviation: 0.0271\n",
    "\n",
    "Detailed Statistics:\n",
    "Precision - Mean: 0.9734, Std: 0.0298\n",
    "Recall - Mean: 0.9790, Std: 0.0319\n",
    "F1 - Mean: 0.9757, Std: 0.0271\n",
    "\n",
    "## yolo12x max_det=700\n",
    "Performance Analysis:\n",
    "Total test cases: 713\n",
    "Good cases (F1 > 0.8): 713 (100.0%)\n",
    "Difficult cases (F1 < 0.5): 0 (0.0%)\n",
    "Average F1 score: 0.9852\n",
    "Median F1 score: 0.9871\n",
    "Standard deviation: 0.0148\n",
    "\n",
    "Detailed Statistics:\n",
    "Precision - Mean: 0.9879, Std: 0.0174\n",
    "Recall - Mean: 0.9826, Std: 0.0161\n",
    "F1 - Mean: 0.9852, Std: 0.0148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. 全体テストの実行（オプション）\n",
    "\n",
    "以下のセルは、すべてのテストデータを対象に詳細な評価を行う場合に実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全体のテストデータに対する評価（時間がかかる場合があります）\n",
    "RUN_FULL_TEST = False  # Trueに変更して全テストを実行\n",
    "\n",
    "if RUN_FULL_TEST and model is not None:\n",
    "    print(\"Running full test on all test data...\")\n",
    "    \n",
    "    full_results = []\n",
    "    full_metrics = []\n",
    "    \n",
    "    for data in tqdm(test_data, desc=\"Full Testing\"):\n",
    "        image_path = data['image_path']\n",
    "        label_path = data['label_path']\n",
    "        image_name = data['image_name']\n",
    "        \n",
    "        # 画像の読み込み\n",
    "        image = Image.open(image_path)\n",
    "        img_width, img_height = image.size\n",
    "        \n",
    "        # グラウンドトゥルースの読み込み\n",
    "        gt_labels = load_yolo_labels(label_path)\n",
    "        gt_boxes = [yolo_to_xyxy(label, img_width, img_height) for label in gt_labels]\n",
    "        \n",
    "        # 推論実行\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image_path)[0]\n",
    "        \n",
    "        # 予測結果の取得\n",
    "        if hasattr(predictions, 'boxes') and predictions.boxes is not None:\n",
    "            pred_boxes = predictions.boxes.data.cpu().numpy()\n",
    "            pred_boxes_with_conf = pred_boxes.tolist()\n",
    "        else:\n",
    "            pred_boxes_with_conf = []\n",
    "        \n",
    "        # 評価\n",
    "        metric = evaluate_predictions(pred_boxes_with_conf, gt_boxes)\n",
    "        full_metrics.append(metric)\n",
    "        \n",
    "        # 結果を保存\n",
    "        full_results.append({\n",
    "            'image_name': image_name,\n",
    "            'image_path': image_path,\n",
    "            'image_size': (img_width, img_height),\n",
    "            'predictions': pred_boxes_with_conf,\n",
    "            'ground_truth': gt_boxes,\n",
    "            'metrics': metric\n",
    "        })\n",
    "    \n",
    "    # 全体の評価指標を計算\n",
    "    if full_metrics:\n",
    "        full_overall_metrics = {\n",
    "            'precision': np.mean([m['precision'] for m in full_metrics]),\n",
    "            'recall': np.mean([m['recall'] for m in full_metrics]),\n",
    "            'f1': np.mean([m['f1'] for m in full_metrics])\n",
    "        }\n",
    "        \n",
    "        print(\"\\\\nFull Test Overall Metrics:\")\n",
    "        for k, v in full_overall_metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        \n",
    "        # 結果をCSVで保存\n",
    "        results_df = []\n",
    "        for result in full_results:\n",
    "            results_df.append({\n",
    "                'image_name': result['image_name'],\n",
    "                'precision': result['metrics']['precision'],\n",
    "                'recall': result['metrics']['recall'],\n",
    "                'f1': result['metrics']['f1'],\n",
    "                'true_positives': result['metrics']['true_positives'],\n",
    "                'false_positives': result['metrics']['false_positives'],\n",
    "                'false_negatives': result['metrics']['false_negatives'],\n",
    "                'num_predictions': len(result['predictions']),\n",
    "                'num_ground_truth': len(result['ground_truth'])\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results_df)\n",
    "        results_csv_path = \"character_detection_test_results.csv\"\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "        print(f\"\\\\nResults saved to: {results_csv_path}\")\n",
    "    \n",
    "elif RUN_FULL_TEST:\n",
    "    print(\"Cannot run full test: Model is not loaded.\")\n",
    "else:\n",
    "    print(\"Full test is disabled. Set RUN_FULL_TEST = True to run the complete evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、文字位置検出モデルの性能評価を行いました。\n",
    "\n",
    "### 使用方法：\n",
    "1. `train_character_detection.py`で学習を完了させる\n",
    "2. 上部の`experiment_dir`を実際の学習結果ディレクトリに変更する\n",
    "3. 各セルを順番に実行して評価とテスト結果の可視化を行う\n",
    "\n",
    "### 評価指標：\n",
    "- **Precision**: 予測した文字位置のうち、正しかった割合\n",
    "- **Recall**: 実際の文字位置のうち、正しく検出できた割合  \n",
    "- **F1 Score**: PrecisionとRecallの調和平均\n",
    "\n",
    "### 出力ファイル：\n",
    "- `character_detection_test_results.csv`: 詳細な評価結果（全体テスト実行時）\n",
    "\n",
    "### 注意事項：\n",
    "- IoU閾値は0.5で設定されています（必要に応じて調整可能）\n",
    "- 文字位置検出は単一クラス（class_id=0）の物体検出として実装\n",
    "- 大量のテストデータがある場合は、全体テストの実行に時間がかかる場合があります"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
