{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Extraction モデルのテスト\n",
    "\n",
    "このnotebookでは、学習済みのline extractionモデルの性能評価と結果の可視化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/project/kuzushiji-vision-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "# 警告を無視\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPUが利用可能な場合は使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 実験設定の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験ディレクトリの設定\n",
    "experiment_dir = \"experiments/line_extraction/20250429_161507\"\n",
    "model_path = os.path.join(experiment_dir, \"weights/best.pt\")\n",
    "config_path = os.path.join(experiment_dir, \"config.yaml\")\n",
    "\n",
    "\n",
    "# 設定ファイルの読み込み\n",
    "def load_yaml(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "config = load_yaml(config_path)\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Model path: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ前処理用の関数\n",
    "def preprocess_image(image_path, input_size=(640, 640)):\n",
    "    \"\"\"画像の前処理を行う\n",
    "\n",
    "    Args:\n",
    "        image_path (str): 画像ファイルのパス\n",
    "        input_size (tuple): 入力サイズ\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 前処理済みの画像テンソル\n",
    "        tuple: 元の画像サイズ\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_size = image.size\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.75696, 0.71561, 0.63938], std=[0.19681, 0.20038, 0.24713]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform(image).unsqueeze(0), original_size\n",
    "\n",
    "\n",
    "# アノテーションの読み込みと前処理\n",
    "def load_and_preprocess_annotations(annotation_file):\n",
    "    \"\"\"アノテーションファイルを読み込み、前処理を行う\n",
    "\n",
    "    Args:\n",
    "        annotation_file (str): アノテーションファイルのパス\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 前処理済みのアノテーションデータ\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(annotation_file)\n",
    "\n",
    "    # boxカラムを文字列からリストに変換\n",
    "    df[\"box_in_original\"] = df[\"box_in_original\"].apply(ast.literal_eval)\n",
    "\n",
    "    # original_imageからファイル名のみを抽出\n",
    "    df[\"image_name\"] = df[\"original_image\"].apply(lambda x: os.path.basename(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# テストデータのパスを設定\n",
    "test_image_dir = \"data/yolo_dataset_page_images_by_book/train/images\"\n",
    "test_annotation_file = \"data/processed/column_info.csv\"\n",
    "\n",
    "# アノテーションの読み込み\n",
    "if os.path.exists(test_annotation_file):\n",
    "    annotations_df = load_and_preprocess_annotations(test_annotation_file)\n",
    "    print(f\"Loaded {len(annotations_df)} annotations\")\n",
    "\n",
    "    # 画像ファイルの存在確認\n",
    "    image_paths = annotations_df[\"original_image\"].unique()\n",
    "    existing_images = [f for f in os.listdir(test_image_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "    existing_images = [image_path for image_path in image_paths if any(image_path.endswith(b) for b in existing_images)]\n",
    "    print(f\"Found {len(existing_images)} test images\")\n",
    "else:\n",
    "    print(f\"Warning: Annotation file not found at {test_annotation_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルのセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOモデルの読み込み\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(model_path)\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 推論と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"IoUを計算する\n",
    "\n",
    "    Args:\n",
    "        box1: [x1, y1, x2, y2]\n",
    "        box2: [x1, y1, x2, y2]\n",
    "\n",
    "    Returns:\n",
    "        float: IoUスコア\n",
    "    \"\"\"\n",
    "    # 交差領域の座標を計算\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # 交差領域の面積を計算\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "    # それぞれのボックスの面積を計算\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # IoUを計算\n",
    "    union = box1_area + box2_area - intersection\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, ground_truth, iou_threshold=0.6):\n",
    "    \"\"\"予測結果を評価する\n",
    "\n",
    "    Args:\n",
    "        predictions: 予測された矩形のリスト [x1, y1, x2, y2, conf]\n",
    "        ground_truth: 正解の矩形のリスト [x1, y1, x2, y2]\n",
    "        iou_threshold: IoUの閾値\n",
    "\n",
    "    Returns:\n",
    "        dict: 評価指標\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = len(ground_truth)\n",
    "\n",
    "    # 各予測に対して最も近い正解を探す\n",
    "    matched_gt = set()\n",
    "    for pred in predictions:\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "\n",
    "        for i, gt in enumerate(ground_truth):\n",
    "            if i in matched_gt:\n",
    "                continue\n",
    "\n",
    "            iou = calculate_iou(pred[:4], gt)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = i\n",
    "\n",
    "        if best_iou >= iou_threshold:\n",
    "            true_positives += 1\n",
    "            matched_gt.add(best_gt_idx)\n",
    "            false_negatives -= 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    # 評価指標の計算\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "    }\n",
    "\n",
    "\n",
    "# テスト画像に対して推論と評価を実行\n",
    "results = []\n",
    "metrics = []\n",
    "\n",
    "for image_path in tqdm(existing_images, desc=\"Testing\"):\n",
    "    image_name = os.path.basename(image_path)\n",
    "\n",
    "    # 推論\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_path)[0]\n",
    "\n",
    "    # 予測結果の取得\n",
    "    pred_boxes = predictions.boxes.data.cpu().numpy()\n",
    "\n",
    "    # グラウンドトゥルースの取得\n",
    "    gt_boxes = np.array(list(annotations_df[annotations_df[\"image_name\"] == image_name][\"box_in_original\"]))\n",
    "\n",
    "    # 評価\n",
    "    metric = evaluate_predictions(pred_boxes, gt_boxes)\n",
    "    metrics.append(metric)\n",
    "\n",
    "    # 結果を保存\n",
    "    results.append(\n",
    "        {\n",
    "            \"image_name\": image_name,\n",
    "            \"image_path\": image_path,\n",
    "            \"predictions\": pred_boxes,\n",
    "            \"ground_truth\": gt_boxes,\n",
    "            \"metrics\": metric,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 全体の評価指標を計算\n",
    "overall_metrics = {\n",
    "    \"precision\": np.mean([m[\"precision\"] for m in metrics]),\n",
    "    \"recall\": np.mean([m[\"recall\"] for m in metrics]),\n",
    "    \"f1\": np.mean([m[\"f1\"] for m in metrics]),\n",
    "}\n",
    "\n",
    "print(\"\\nOverall Metrics:\")\n",
    "for k, v in overall_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST Overall Metrics:\n",
    "precision: 0.8600\n",
    "recall: 0.9050\n",
    "f1: 0.8724\n",
    "\n",
    "TRAIN Overall Metrics:\n",
    "precision: 0.8723\n",
    "recall: 0.8955\n",
    "f1: 0.8754"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(result):\n",
    "    \"\"\"検出結果を可視化する\n",
    "\n",
    "    Args:\n",
    "        result: 結果辞書\n",
    "    \"\"\"\n",
    "    image = Image.open(result[\"image_path\"])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # 予測結果の描画（赤）\n",
    "    for box in result[\"predictions\"]:\n",
    "        x1, y1, x2, y2, conf, _ = box\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color=\"red\", linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.text(x1, y1 - 5, f\"{conf:.2f}\", color=\"red\")\n",
    "\n",
    "    # 正解の描画（緑）\n",
    "    for box in result[\"ground_truth\"]:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color=\"green\", linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Image: {result['image_name']}\\nF1: {result['metrics']['f1']:.4f}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 最良と最悪のケースを可視化\n",
    "sorted_results = sorted(results, key=lambda x: x[\"metrics\"][\"f1\"], reverse=True)\n",
    "\n",
    "print(\"Best Case:\")\n",
    "best_case = sorted_results[3]\n",
    "visualize_results(best_case)\n",
    "\n",
    "print(\"\\nWorst Case:\")\n",
    "worst_case = sorted_results[-1]\n",
    "visualize_results(worst_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. エラー分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1スコアの分布を可視化\n",
    "f1_scores = [r[\"metrics\"][\"f1\"] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(f1_scores, bins=20)\n",
    "plt.title(\"Distribution of F1 Scores\")\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 困難なケースの分析\n",
    "difficult_cases = [r for r in results if r[\"metrics\"][\"f1\"] > 0.8 and r[\"metrics\"][\"f1\"] < 1.0]\n",
    "print(f\"\\nFound {len(difficult_cases)} difficult cases (F1 < 0.5)\")\n",
    "\n",
    "if difficult_cases:\n",
    "    print(\"\\nAnalyzing a sample of difficult cases:\")\n",
    "    for case in difficult_cases[:5]:\n",
    "        print(f\"\\nImage: {case['image_name']}\")\n",
    "        print(f\"Metrics: {case['metrics']}\")\n",
    "        visualize_results(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
