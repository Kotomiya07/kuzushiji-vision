**提案1: 文脈・構造適応型 Vision Transformer (CSA-ViT: Context and Structure Aware Vision Transformer) - 詳細**

**1. コンセプトと目標**

CSA-ViTは、Vision Transformer (ViT) [6, 20] の強力な画像表現学習能力を基盤としつつ、日本語くずし字認識特有の課題、すなわち「**多様な字形（構造）**」と「**文脈依存性（周辺文字・レイアウト）**」に正面から取り組むことを目的としたアーキテクチャです。個々の文字形状だけでなく、その文字が置かれている状況（どの文字の隣にあるか、文書のどこにあるか）や、文字内部の微細な構造（ストロークの流れ、連結部）をより深く理解することで、認識精度の大幅な向上を目指します。

**2. アーキテクチャ詳細**

CSA-ViTは主に「入力モジュール」「エンコーダ」「デコーダ」の3つの部分から構成されます。

**(a) 入力モジュール**

*   **主入力:** 認識対象の文字（または文字が含まれる領域）の画像。ViTの標準的な処理に従い、画像を固定サイズのパッチに分割し、各パッチを線形層でベクトル（パッチ埋め込み）に変換します。位置エンコーディング（学習可能または固定）が付与され、パッチの空間的位置情報が保持されます。
*   **補助入力:**
    1.  **周辺文脈情報 (Context Embedding):**
        *   **取得方法:** 認識対象文字の周辺領域の画像を取得します。これは、対象文字を含むより大きなウィンドウ領域を低解像度でエンコードするか、隣接する文字領域の画像を別途入力します。
        *   **処理:** 別の軽量CNNやViT、あるいは単純な線形層で周辺文脈画像から特徴ベクトル（文脈埋め込み）を抽出します。この埋め込みは、後述するエンコーダ内のCross-Attentionで使用されます。
        *   **目的:** 隣接する文字の形状や種類に関する情報を提供し、変体仮名の決定や草書の連結部分の解釈を助けます。
    2.  **レイアウト情報 (Layout Embedding):**
        *   **取得方法:** 事前のレイアウト解析（古典的な手法または軽量な深層学習モデル）により、文書画像中のテキスト行や文字ブロックの位置情報を取得します。あるいは、文書全体を対象とする場合、文字候補領域間の関係性（隣接、上下など）をグラフ構造で表現します。
        *   **処理:** 取得したレイアウト情報をGraph Neural Network (GNN) [2, 5, 7, 11, 13]、特にGraph Attention Network (GAT) [2, 5] などを用いて処理し、各文字（または領域）に対応するレイアウト埋め込みベクトルを生成します。
        *   **目的:** 散らし書きや複数段組、注釈など、複雑なレイアウトにおける文字の相対的な位置関係や読み順に関する情報を提供します。

**(b) エンコーダ (CSA-ViT Encoder)**

標準的なViTエンコーダブロック（Multi-Head Self-Attention (MHSA) + FeedForward Network (FFN)）をベースに、以下の改良を加えます。

*   **Transformerブロック構成:** 各ブロックは以下のモジュールで構成されます（順序や組み合わせは調整可能）。
    1.  **構造適応モジュール (Structure-Aware Module):**
        *   **目的:** MHSAが捉えきれない可能性のある、くずし字の微細なストローク形状、連結部、部首などの局所的・構造的特徴を補強します。
        *   **実装例1 (CNNブランチ):** 各パッチ埋め込みに対し、小さなカーネルサイズを持つ軽量な畳み込み層（例: Depthwise Separable Convolution）を並列に適用し、局所パターン特徴を抽出します。抽出された特徴は、元のパッチ埋め込みに加算または連結されます。
        *   **実装例2 (Local GAT):** 各パッチを中心として、近傍のパッチをノードとする局所的なグラフを構築します。エッジは空間的距離や特徴類似度で重み付けされます。この局所グラフ上でGraph Attention Network (GAT) を適用し、パッチ間の微細な関係性（例: ストロークの繋がり）をモデル化します。計算コストを考慮し、限定的な範囲や層での適用を検討します。
    2.  **Multi-Head Self-Attention (MHSA):** 標準的なViTと同様に、パッチ埋め込み（構造適応モジュールで強化されたもの）間の関係性を学習し、大域的な形状特徴を捉えます。
    3.  **文脈統合モジュール (Context Integration Module):**
        *   **目的:** 補助入力（周辺文脈埋め込み、レイアウト埋め込み）からの情報を、画像特徴に統合します。
        *   **実装 (Cross-Attention):** MHSAの後（または並列）にMulti-Head Cross-Attention層を挿入します。Query (Q) には画像パッチの埋め込みを、Key (K) と Value (V) には補助入力の埋め込み（文脈埋め込み、レイアウト埋め込み、またはそれらを結合したもの）を使用します。これにより、各画像パッチが、自身の文脈（周辺文字やレイアウト上の位置）を考慮した表現へと更新されます。
    4.  **FeedForward Network (FFN):** 各パッチ表現を非線形変換します。

*   **レイヤー正規化 (Layer Normalization) と残差接続 (Residual Connection):** 標準的なTransformerと同様に、各モジュールの前後で適用し、学習を安定化させます。

**(c) デコーダ**

エンコーダが出力した文脈・構造情報を統合した特徴量シーケンスを受け取り、最終的な文字（Unicode）シーケンスを出力します。

*   **選択肢1: Transformer デコーダ:**
    *   **仕組み:** 自己回帰的に、前のステップで予測した文字とエンコーダからの画像特徴の両方を考慮して次の文字を予測します。Attention機構（Self-AttentionとEncoder-Decoder Attention）により、長期的な依存関係や言語的な自然さをモデル化しやすいです。
    *   **利点:** 文脈（既に出力された文字）を利用した予測が可能。言語モデルとしての側面も持つ。
    *   **欠点:** 逐次的な生成のため、推論速度が遅くなる可能性がある。
    *   **文脈利用強化:** デコーダのAttention機構が、エンコーダ出力（画像特徴全体）と既出力文字埋め込みの両方を効果的に参照できるように設計します。
*   **選択肢2: Connectionist Temporal Classification (CTC) デコーダ:**
    *   **仕組み:** エンコーダが出力した特徴量シーケンスの各タイムステップ（各パッチに対応）に対して、文字（＋空白記号）の確率分布を出力します。後処理（Best Path Decodingなど）で重複文字や空白記号を除去し、最終的な文字シーケンスを得ます。
    *   **利点:** 各タイムステップの予測が独立しているため、並列計算が可能で高速な推論が期待できます。文字のアライメントを明示的に学習する必要がありません。
    *   **欠点:** 各ステップの予測が局所的であり、言語的な長期依存関係のモデル化は不得意。後処理で言語モデルを別途適用することが多い。
*   **選択肢3: ハイブリッドアプローチ:** CTC損失とTransformerデコーダ（Attentionベース）の損失を組み合わせて学習し、両方の利点を活かすことも考えられます [20]。

**3. 画期的な点と期待される効果**

*   **構造と文脈の明示的統合:** 従来のViTが主に大域的な画像特徴に焦点を当てるのに対し、CSA-ViTは「構造適応モジュール」で局所的な字形特徴（ストローク、連結部）を、「文脈統合モジュール」で周辺文字やレイアウトという文脈情報を、それぞれ明示的にモデルに組み込みます。これは、多様な崩し字や異体字、そして文脈によって意味や形が変わるくずし字の特性に直接対応するアプローチです。
*   **ViTの能力の活用と拡張:** ViTの持つ強力な大域的特徴抽出能力を維持しつつ、くずし字特有の課題に対応するための専門モジュールを追加することで、単純なViT適用よりも高い精度と頑健性を目指します。
*   **課題解決への貢献:**
    *   **多様な字形・異体字:** 構造適応モジュールとViT本体の組み合わせにより、多様な崩しパターンや異体字に対する不変的な特徴表現の獲得を促進します。
    *   **連結文字・重なり文字:** 構造適応モジュール（特にLocal GAT）が連結部分の微細な特徴を捉え、文脈統合モジュールが隣接文字からの情報を加味することで、切り離しが難しい文字の認識を改善します。
    *   **文脈依存性・変体仮名:** 文脈統合モジュールが周辺文字の情報を直接利用するため、文脈によって判断される変体仮名の認識精度向上が期待されます。
    *   **複雑なレイアウト:** レイアウト埋め込みを統合することで、文字の絶対的な形状だけでなく、文書内での相対的な位置関係も考慮した認識が可能になり、散らし書きなどへの対応力が向上します。

**4. 学習戦略**

*   **事前学習 (Self-Supervised Learning):** 大量のラベルなし古文書画像データを用いて、自己教師あり学習を行います。
    *   **Masked Autoencoder (MAE) [14, 19]:** 画像パッチの大部分をマスクし、エンコーダに通した後、軽量なデコーダで元のピクセル値を復元するように学習します。これにより、くずし字の形状パターンに関する汎用的な表現を獲得します。CSA-ViTのエンコーダ部分を事前学習します。
    *   **Contrastive Learning (SimCLR, MoCo等 [15, 22, 24]):** 同じ文字の異なるインスタンス（異なる崩し方、データ拡張されたもの）を正例ペア、異なる文字を負例ペアとし、埋め込み空間上で正例ペア間の距離を小さく、負例ペア間の距離を大きくするように学習します。異体字や崩しに対する頑健な表現獲得に有効です。
*   **ファインチューニング (Supervised Learning):** ラベル付きのくずし字データセット（例: Kuzushiji-MNIST, Kuzushiji-49, Kuzushiji-Kanji [10], 国文研データセット [1] など）を用いて、事前学習済みのモデル全体（または一部）をファインチューニングします。
    *   エンコーダの上に選択したデコーダ（TransformerまたはCTCヘッド）を接続し、文字認識タスク（分類または系列生成）を解くように学習させます。
    *   学習率を小さめに設定し、事前学習で獲得した知識を壊さないように調整します。

**5. 実装上の考慮事項**

*   **計算コスト:** ViT自体が計算コストの高いモデルであり、さらに補助入力処理（GNN等）や追加モジュール（構造適応、Cross-Attention）が加わるため、効率的な実装（混合精度学習、勾配チェックポイントなど）や、適切なモデルサイズ（Base, Small, Tinyなど）の選択が必要です。
*   **データ要件:** 補助入力として周辺文脈やレイアウト情報を使用する場合、それらの情報を抽出・生成するための前処理パイプラインやアノテーションが必要になる場合があります。
*   **ハイパーパラメータ:** Transformerの層数、ヘッド数、埋め込み次元、補助入力の次元、構造適応モジュールの設計、学習率スケジュールなど、調整すべきハイパーパラメータが多くなります。

**6. 従来研究との比較**

*   **CNNベース:** CNNは局所的な特徴抽出に優れますが、受容野の制約から大域的な形状や長距離の文脈依存関係を捉えるのがViTほど得意ではありません。CSA-ViTはViTベースで大域的特徴を捉えつつ、CNN的な局所特徴も構造適応モジュールで補強します。
*   **CNN+RNN+Attention:** 文字画像をCNNで特徴抽出し、RNN（LSTM/GRU）で系列性をモデル化、Attentionで文脈を見る構成が一般的でした。CSA-ViTはViTにより画像全体から直接的に空間的・構造的関係性を捉え、さらに補助入力で外部文脈を直接統合する点で異なります。
*   **標準ViT:** 標準的なViTをくずし字認識に適用する試みもありますが [20]、CSA-ViTはくずし字特有の「構造」と「文脈」の課題に特化したモジュールを追加することで、より高い性能を目指します。

**結論**

CSA-ViTは、ViTの表現力を最大限に活かしながら、くずし字認識の核心的課題である字形の多様性と文脈依存性に特化した改良を加えたアーキテクチャです。構造適応モジュールと文脈統合モジュールという新しい要素を導入することで、従来モデルでは困難だったレベルでの精度向上と、より多様な古文書資料への対応可能性を秘めています。自己教師あり学習による事前学習と組み合わせることで、ラベルデータが限られる状況でも高い性能を発揮することが期待されます。
